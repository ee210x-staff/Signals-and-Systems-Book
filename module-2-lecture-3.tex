\section{Module 2: Lecture 3\\Signals and Vectors}

%\subsection{Introduction}
%In the previous lecture, we have learnt how a periodic input given to a Linear Shift Invariant system results in a periodic output with the same period. Now in this lecture we will see some new concepts by which every input can be expressed as sinusoidal inputs. Henceforth, it will be simpler to analyse their outputs.

\subsection{Relations between Signals and Vectors}
Consider a 2-dimensional vector, we can decompose this vector into two perpendicular components with $e_1$ and $e_2$ as the unit vectors along those directions. To find these components, we need to use Dot Product.
\subsubsection{Dot Product}
	Dot Product of two vectors u and v is defined as the magnitude of vector u multiplied by the magnitude of v multiplied by cosine of the angle between these two vectors.
    
	    					\begin{equation*}u.v = |u||v|cos\theta\end{equation*}
                            
where $\theta$ is the angle between vectors u and v. If u is a unit vector, then the dot product of vectors v and u gives the component of vector v in the direction of u, with the value |v|cos$\theta$.
	\begin{figure}[ht]
\centering
\includegraphics[scale=0.08]{figure.jpg}
\end{figure}
    
    
    Consider the above figure, here OB is a vector denoted by v which is expressed as
    \begin{equation*}v = v_1e_1 + v_2e_2\end{equation*}
    
    where $v_1$ and $v_2$ are the components in $e_1$ and $e_2$ directions respectively.
By this we can say that vectors $e_1$ and $e_2$ spans the space in 2D i.e. we can express any vector as a linear combination of e1 and e2. A collection of vectors span a space. Suppose if any vector can be expressed uniquely as a linear combination of $\{ u_1$,$u_2$,$u_3$ .... $u_n \}$, where $\{ u_1$,$u_2$,$u_3$...$u_n \}$ are linearly independent, then $\{ u_1$,$u_2$...$u_n \}$ is said to form a basis.\\
\noindent
Also, if $\langle u_i$,$u_j \rangle$ = 0 for i,j belonging from 1 to n,i.e. dot product of any two vectors is zero,  then the basis is said to be an \textit{orthogonal basis}. Finally, for a n-dimensional space, a collection of n linearly independent vectors forms the basis.\\
\noindent
The dot product of two vectors $u$ and $v$ is equal to the sum of the products of the corresponding perpendicular components. Suppose

				\begin{equation*}u = u_1e_1 + u_2e_2\end{equation*}
                        
                        \begin{equation*}v = v_1e_1 + v_2e_2\end{equation*} 
\noindent                        
                        therefore,\begin{equation*} u.v = u_1v_1 + u_2v_2\end{equation*}
           
\subsubsection{Dot Product of Discrete Sequences}           
Now, consider a discrete sequence with 2 non-zero points. This sequence can be compared to a 2-dimensional vector v with $v_1$ and $v_2$ as its perpendicular components such that it is equal to the values at the 2 non zero points of the sequence. Similarly for a sequence with n non-zero points can be considered as a vector in n dimensional space. Also, the concept of dot product is similarly applied to the discrete sequences.

For example : Consider
	x[n] = $(1/2)^n$ u[n];
    
    i.e.
     \begin{equation*}
    x[n] = (1/2)^n	\enspace	\enspace for\enspace	 n\geq0\end{equation*}
    \begin{equation*}	 0	\enspace  \enspace	 for\enspace	 n<0\end{equation*}
               
	 \begin{equation*} y[n] = (1/3)^nu[n] \end{equation*}
    
    i.e. 
    \begin{equation*}y[n] = (1/3)^n	\enspace \enspace	for\enspace n\geq0\end{equation*} \begin{equation*}0\enspace \enspace			for\enspace n<0 \end{equation*}
                  
                  
  Lets calculate the dot product of x[n] and y[n],i.e. summing the product of corresponding components.
  We have,
   \begin{equation*} \langle x[n],y[n] \rangle = \sum_{n=-\infty}^{\infty}\ x[n]y[n]\end{equation*} 
   \begin{equation*} \langle x[n],y[n] \rangle = \sum_{n=0}^{\infty}\ x[n]y[n]\end{equation*} 
   \begin{equation*} \langle x[n],y[n] \rangle = \sum_{n=0}^{\infty}\ (1/2)^n*(1/3)^n\end{equation*} 
   \begin{equation*} \langle x[n],y[n] \rangle = \sum_{n=0}^{\infty}\ (1/6)^n\end{equation*} 
   \begin{equation*} \langle x[n],y[n] \rangle = \frac{1}{(1-(1/6))}\end{equation*} 
   \begin{equation*} \langle x[n],y[n] \rangle = \frac{6}{5}\end{equation*} 
  		
          
 	By this we compute the dot product of two discrete sequences. This dot product is also called as \textit{Inner Product}. Inner product of two vectors $u$ and $v$ is represented by $\langle u,v \rangle$.

\subsubsection{Inner Product of Continuous Signals}
       In a discrete sequence, a unit vector can be expressed as $\delta[n-N]$ for all integers Z. A n-dimensional vector v can be expressed as
        \begin{equation*}v = v_1e_1 + v_2e_2 + ........ + v_ne_n\end{equation*}
        Now writing the above equation in terms of sequences, we have
       
    	\begin{equation*}	x[n] = 	\sum_{n=-\infty}^{\infty}\ x[N]\delta[n-N]\end{equation*}
            
            where x[n] is the component along dimension N.
       
Similarly applying the same concept for continuous time functions, we need to replace summation by integral, which gives,

					\begin{equation*}x(t) = \int_{-\infty}^{\infty} \! x(\lambda)\delta[t-\lambda] \ \mathrm{d}\lambda\end{equation*}
                    
             where x($\lambda$) is the component in direction $\lambda$ and $\delta[t-\lambda]$ is continuous impulse at $\lambda$ similar to unit vector.So inner product of x(t) and y(t) is given by
             
           \begin{equation*} \langle x(t),y(t) \rangle = \int_{-\infty}^{\infty} \! x(t)y(t) \ \mathrm{d} t\end{equation*}
             
 \subsection{Sinusoids with same period}
          Let us see how we can express any signal as sinusoidal functions. So, first we need to find sinusoidal signals which are perpendicular. Consider a signal x(t) which is periodic with period T,i.e. \begin{equation*}  x(t) = x(t+T)\end{equation*}. Assume that it can be expressed as a sum of sinusoidal functions. We should take a sinusoidal function which is periodic with period T. So it is of the form $A_k\cos (\frac{2\pi}{T}kt + \phi_k)$. We have,
          
          \begin{equation*}x(t) = \sum_{k=-\infty}^{\infty}\ A_k\cos (\frac{2\pi}{T}kt + \phi_k)\end{equation*}
          
          Let's take two different \textit{k}
.  \begin{equation*}
x_1 (t) = A_1 \cos (\frac{2\pi}{T}k_1t + \phi_1)
\end{equation*}
           and \begin{equation*}
x_2 (t) = A_2 \cos (\frac{2\pi}{T}k_2t + \phi_2)
\end{equation*}
          Consider the inner product of $x_1(t)$ and $x_2(t)$ with the interval going from 0 to T. We are here restricting our interval to T because the integral might diverge when we integrate from 0 to $\infty$. We have,
           \begin{equation*}\langle x_1(t),x_2(t) \rangle = \int_{0}^{T} \! x_1(t)x_2(t) \ \mathrm{d}t\end{equation*}
           \begin{equation*}\langle x_1 (t),x_2 (t) \rangle = \int_{0}^{T} \! \cos (\frac{2\pi}{T}k_1t + \phi_1) \cos (\frac{2\pi}{T}k_2t + \phi_2) \ \mathrm{d}t\end{equation*}
           Using 
          \begin{equation*} 2cosAcosB = cos(\frac{A+B}{2}) + cos(\frac{A-B}{2})\end{equation*}
          We have,
          \begin{equation*}\langle x_1 (t),x_2 (t) \rangle = \int_{0}^{T} \! \left\lbrace \cos (\frac{2\pi}{T}\frac{(k_1+k_2)}{2}t + \frac{\phi_1+\phi_2}{2}) + \cos (\frac{2\pi}{T}\frac{(k_1-k_2)}{2}t + \frac{\phi_1-\phi_2}{2}) \right\rbrace \ \mathrm{d}t \end{equation*}
          
          
        
        
                
                If ($k_1$ + $k_2$) or ($k_1$ - $k_2$) are not zero, that means we have a finite number of cycles of the sinusoids, implying the integral is zero. However if $k_1$=$k_2$, the second integral becomes T times $\cos (\theta_1-\theta_2)$.
                Thus we have,
            \begin{equation*} \langle x_1 (t),x_2 (t) \rangle  \neq 0	\enspace \enspace		for \enspace k_1 = k_2\end{equation*}
            \begin{equation*} \langle x_1 (t),x_2 (t) \rangle = 0	\enspace \enspace		for \enspace k_1 \neq k_2\end{equation*}
                
                
                Hence, we have proved that two sinusoids with same time period are perpendicular if they don't have same angular frequency and vice-versa. Thus using this important concept, we will be able to write any signal as sum of sinusoidal signals and hence analysis of these signals will be simpler.

